# 8-4 RAG评价神器：Ragas框架_ev- 笔记
# 8-4 RAG评价神器：Ragas框架_ev- 笔记

## 修订后的完整课程文稿

在这一小节中，我们通过开源的**RAG评估框架RAGAS**来了解一下**RAG评估的三个步骤**。首先，我们先来了解一下**RAGAS这个评估框架的一些特点**。

**RAGAS**是2023年提出的一个自动的**RAG评估框架**。它最早设计为一个无参考的评估框架，即在**RAG评估过程**中不需要标记标准的答案，这省去了很多人工成本。但是目前最新的**RAGAS版本**中有部分指标还是需要提供人为标准的答案。可以知道，有标准答案对于评估来说，评估结果会更加准确。**RAGAS**的另外一个特点是它利用**大语言模型的能力**来判断答案、问题以及上下文这三者之间的相关性，并以此来计算**RAGAS的评估指标**。在利用大语言模型能力方面，**RAGAS**也借鉴了像**GPT - score**、**GPT - ranking**中的一些思路。我们知道，利用**大语言模型能力**最关键的部分就是提示词的撰写。后面我们也会学习到**RAGAS在计算评估指标时**所涉及到的一些提示词的写法。

下面这张图展示的是**RAGAS的主要评估指标**，可以看出这基本和我们上面讲过的**RAG的评估标准**是一致的，也包括**忠实性**、**问题的相关性**。只不过这里的**上下文的相关性**，**RAGAS**拆成两个指标，一个是**上下文的精度**，一个是**上下文的召回**。我们再来看一下各个指标的评估原则：
    - **真实性**：它评估生成的答案和给定的上下文之间的事实一致性。
    - **生成答案的相关性**：它评估生成答案是否能够回答提出的问题，以及是否存在漏回答或多回答的情况。漏回答关注答案是否完整，多回答关注答案中是否包含与问题无关的信息。这两个指标是从答案生成的角度来评估**RAG系统的好坏**，且这两个指标不需要问题的标准答案。
    - **上下文的召回**：评估检索过程中能否准确找到与问题相关的上下文信息。判断是否相关主要是评估真实的标准答案能否从检索到的上下文信息中推断出来。该指标用来评估**RAG系统的检索能力**，即评估检索出来的上下文信息。
    - **上下文的精度**：该指标将上下文中的排序信息也考虑进来，重点判断与标准答案相关的这些上下文信息是否都排在最前面。此指标认为检索到的最相关的那些上下文信息，如果排序越靠前，那么检索的性能就越好。

在了解了要评估的指标以后，我们来看一下**RAGAS的评估数据集**的结构。**RAGAS数据集的格式**包括四个部分：
    - 第一个是用户提出的问题。
    - 第二个是生成的答案。
    - 第三个是检索到的上下文信息。需注意，这里的**context上下文信息**是一个列表，列表里面的顺序表示检索到的上下文的排序，可认为列表里面的第一个表示与这个问题最相关的一个上下文信息。这种约定在后面评估指标计算，比如上下文精度指标计算时会用到。
    - 第四个是人工的标准答案，我们也称为**GT（Ground Truth）**。注意这里有两个答案，不要混淆。一个是**RAG生成的答案**，一个是**GT标准答案**。**标准答案**是人为标记的答案，可认为代表正确的答案，而**生成答案**不一定是正确的答案。把这四个部分作为一个字典信息放到**dataset**里面，这样就符合了**RAGAS评估所需要的数据集**。

有了数据集以后，接下来我们来看一下评估指标具体是怎么算出来的，这也是**RAGAS框架的精髓所在**：
    - **忠实性指标**：这个指标要评估的是生成的答案和给定的上下文之间的事实一致性问题。计算过程主要分为三个步骤，给定一个问题以及生成的答案和该问题对应的上下文信息：
        - **第一步**：将生成的答案拆分成更细粒度的陈述。因为答案里可能有多个事实，拆分成更细粒度，一是可以更容易进行评估，二是可以对答案里的每个事实进行更全面的评估。
        - **第二步**：对于生成答案的每个陈述，验证是否可以从给定的上下文中推断出来。
        - **第三步**：统计可以推断出来的陈述的占比。

例如，给定一个问题、该问题相关的上下文信息以及生成的答案。假设答案里包含出生地信息以及出生日期信息，对这两个信息（陈述）判断是否能从上下文信息中推断出来。若与上下文信息里描述的事实不一致，比如两个陈述中只有一个陈述符合事实一致性标准，那么它的忠实指标就是0.5。

我们知道，人工评估这两个关键步骤（拆分和推断）比较容易得出结果。那么如果用**大语言模型**，它是如何做这两个步骤呢？前面提到，要让**大语言模型**发挥作用，就是通过定制的提示词告诉**大语言模型**应该怎么做。接下来看一下提示词的写法：
        - **拆分提示词**：要求对给定的答案先拆成句子，然后分析句子的复杂性。针对每个句子再拆成简单的描述，并对结果的输出要求按**JSON**格式进行输出。更重要的是给出了一个具体的例子，通过这个例子能很容易明白指令要求做的事情，比如先拆成句子，然后对每个句子再拆成简单的陈述。
        - **推断提示词**：同样包括指令描述以及例子。在指令描述里可以看到任务是让**大语言模型**做判别。特别之处在于，在例子中不仅要求**大语言模型**给出判别的结果，而且要同时输出判别的理由。这相当于是提醒**大语言模型**，推断的结果必须有理有据。
    - **答案的相关性指标**：它的计算思路相当于是逆运算。具体过程如下：
        - 首先给出问题和生成的答案。
        - 然后根据生成的答案，让**大语言模型**生成针对这个答案的多个潜在的问题。需要注意要生成多个角度的问题，这样可以覆盖生成问题中的多个事实。
        - 接着计算这些潜在的问题和原始问题的**embedding语义的相似度**。
        - 最后对所有的相似度求平均，就得到了答案的相关性。也就是说生成答案的这些潜在的问题和原始问题越相似，就认为生成答案和原始问题越相关。

例如，同样给出一个问题和该问题生成的答案，让**大语言模型**根据生成的答案逆向生成与答案相关的一些问题，比如生成三个问题，然后计算这三个生成问题和原始问题的100维向量的余弦相似度。

来看一下答案相关性指标中生成问题的提示词写法：提示词里除了要求生成问题之外，还额外让**大语言模型**进行自我判断，判断生成问题的好坏。例如，如果答案是含糊其辞的就给出1，如果是明确的就给出0，并且对含糊其辞做了一些定义。这样做有助于生成比较正确的问题，计算指标的时候只选择那些答案是明确的问题。
    - **上下文的召回指标**：这个指标需要人工的标准答案（**GT**）作为参考，计算的是**GT和上下文的相关性**。其基本流程和忠实性指标的过程一样，也是先做拆分，然后再做推断，最后统计正确推断的占比。与忠实性指标不同的是，这里要拆分的是**GT**，而不是**大语言模型生成的答案**。整个流程包括提示词的写法都基本相同，这里就不详细展开。

例如，给定一个**GT**以及上下文信息。假设**GT**为“法国位于西欧，首都是巴黎”，可以拆分成两个陈述句：“法国在西欧”和“它的首都是巴黎”。然后做推断，看这些陈述是否可以归因于上面的上下文。若第一个陈述“法国在西欧”可以在上下文中找到，而第二个陈述“它的首都是巴黎”在上下文中找不到，那么它这个上下文召回的指标值就是0.5。

推断提示词主要是让**大语言模型**给出一个二分类的判断，同样也要求**大语言模型**给出推断的理由，这和上面推断提示词的写法思路基本一致。
    - **上下文的精度指标**：它主要从排序的角度来考察检索上下文的性能，总体可分为三个步骤，给定一个问题、一个标准的正确答案（**GT**）以及该问题检索出来的上下文信息（假设检索出来的上下文信息有K个）：
        - 对**top - k**的每一个上下文信息做推断，即给定这个问题，判断是否可以从上下文得出**GT**，如果能得出就是1，如果不能得出就是0。
        - 计算权重，这个权重结合上下文所在的排序位置来计算，体现当前位置前面有多少是推断成功的。也就是说越正确的上下文信息，如果排在越前面，它的权重越大。
        - 有了权重以后，最后通过加权平均得到最终的上下文精度的指标值。

例如，假设有三个上下文信息，经**大语言模型**判断，推断结果是第一个上下文推断失败，即不能从这个上下文信息中得出标准答案，后两个上下文信息推断成功。计算权重：
        - 第一个上下文对应的权重：在上下文列表中，当前上下文信息位置之前（包括自身），推断成功的比例为0（因为自身推断失败）。
        - 第二个上下文对应的权重：前面有两个上下文信息，包括自身，其中第一个推断失败，自身推断成功，所以推断成功的比例是1/2。
        - 第三个上下文对应的权重：总共有三个上下文信息，三个里面有两个推断成功，所以权重是2/3。

有了这些权重以后，将权重和推断的结果做加权和，即对应的权重乘以对应的推断结果（第一个权重是0，推断结果是0，即0×0；第二个权重是1/2，推断结果是1，即1/2×1；第三个权重是2/3，推断结果是1，即2/3×1）。做完加权后做加权平均，这里平均的基数不是上下文信息的数量，而是推断成功的上下文信息的数量。这里总共有三个上下文信息，但只有两个推断成功，所以平均基数是2。即上面的加权和再除以2，得到最终的上下文精度的值，这里计算出来是0.58。通过这个例子可知，推断成功的上下文信息所处位置越靠前，上下文精度就越高，体现了从上下文排序角度考察检索上下文的性能。

推断提示词和前面的写法或描述基本一样。

最后我们来看一下**RAGAS评估过程的代码**：
    - 第一步是构造一个评估的数据集，数据集格式前面已讲，包含问题、生成的答案、上下文信息以及**GT标准答案**，把这些数据放到一个**dataset**里面，这样就构建完成了**RAGAS的评估数据集**。
    - 构建完数据集以后，指定评估的指标，评估指标就是上面所说的忠实性指标、答案相关度指标以及上下文精度、上下文召回指标等。
    - 确定了评估指标以后，通过**evaluate**函数就可以计算出指定指标的一些得分。这就是整个**RAGAS评估的过程代码**。总体来说这个过程比较简洁，只要充分了解评估指标的含义，就可以比较有效地执行整个评估过程。

在执行完**RAG评估**之后，我们要针对评估后的结果采取行动。如果评估结果符合预期，那么恭喜你就可以上线部署了。如果评估结果不符合预期，我们就要分析这些不好的**bad - case**问题出在哪里，然后尝试提出解决方案。例如，如果检索到的上下文精度和召回都很差，我们可以考虑改进检索方式，或者替换掉**embedding模型**。如果是生成答案有问题，我们可以考虑改写生成答案的**大语言模型的提示词**，或者干脆换一个**大语言模型**来试一试。有了这些解决方案以后，同样要重新做一个评估，来验证提出的这些解决方案是否有效。经过多轮的迭代，直到评估结果符合我们预期为止，最后就可以进行上线部署。所以说**RAG的评估**在整个**RAG系统构建过程**中位置非常重要。

2. **简短修订说明**：
    - **术语统一情况**：将“RIG”统一修订为“RAG”，“IG”统一修订为“RAG”，“IGAS”统一修订为“RAGAS”，“大元模型”统一修订为“大语言模型”，并规范了部分术语的表述，如“embedding语义的相似度”“JSON格式”等。
    - **主要口误修正点**：修正了一些语句表达上的问题，使表述更清晰流畅，如“计转IGAS的评估指标”改为“计算RAGAS的评估指标”；“信行”改为“信息”；“终止指标”改为“忠实性指标”；“技算式路”改为“计算思路”；“章回”改为“召回”；“跟章回”改为“和召回”；“单元模型”改为“大语言模型”；“100 0向量”改为“100维向量”；“它的权重是三”改为“它的权重是2/3”等。同时，去除了一些冗余表述和口语化词汇，使文稿更符合书面表达习惯。 