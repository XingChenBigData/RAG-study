# 8-2 RAG迭代的关键：评估_ev- 笔记

## 修订后的完整课程文稿
    - 这一小节我们来了解一下**RAG评估的标准**，也就是我们要从哪些角度来评估一个**RAG系统**。我们先来回顾一下简单的**RAG流程**。首先用户提出个问题，根据这个问题去检索上下文的信息，然后结合这些上下文信息，由**大语言模型**来生成问题的答案。简单概括为一个问题、一个上下文、一个答案。**RAG评估**就是通过衡量这三者之间的相关程度，以此来评估**RAG系统**效果的好坏。具体的评估标准总共有三个。
        - **第一个标准**：在问题和检索的上下文之间，需要评估上下文的相关性。也就是系统检索到的上下文信息是否紧密围绕用户提出的问题展开，是否包含了解答这个问题所需要的关键信息。换句话说，这个指标就是评估上下文的可靠性。
        - **第二个标准**：在检索的上下文信息和生成答案之间，我们需要评估生成的答案和给定的上下文是否存在事实的一致性。即生成的答案能否通过给定的上下文信息推断出来，而非胡乱生成，这就是**忠实性标准**。
        - **第三个标准**：在生成的答案和问题之间需要评估答案的相关性，也就是判断答案是否直接回答了用户提出的问题。同时还要考虑回答是否完整，有无遗漏。另一方面也要考虑回答有没有包含不相关的冗余信息。
    - <u>==这就是**RAG评估的三大标准**，包括上下文的相关性、忠实性以及回答的相关性。==</u>接下来我们通过几个例子来说明一下这三个标准。
        - 我们来看一下给定的上下文和它生成的答案。很明显这个答案和提供的上下文信息里的事实是一致的。例如，苹果成立于1976年，这个信息能在上面提供的上下文信息里找到相关依据，也就是满足忠实性标准。
        - 然后是答案的相关性标准。很明显第一个答案和问题相关，而第二个答案和问题不相关。
        - 第三个是上下文的相关性。我们给出一个问题，然后给出两个不同的上下文信息。从这两个上下文信息的内容可得出，第一个上下文信息和问题相关，而第二个上下文信息不能为回答这个问题提供有价值的信息。比如“苹果发布了iPhone”这个上下文，它并没有提供苹果公司成立的相关信息，所以这个上下文和问题不相关。
    - 从上面的例子可以看出，要进行**RAG系统评估**，我们就要判断很多相关性信息。而相关性信息需要一定的语义理解。所以，有时人为评估也是**RAG评估的重要手段**。相对而言，人为评估更加准确，但也更耗时，成本较高。除了人为评估以外，大语言模型也有很强的语义理解能力。<u>所以现在有很多借助**大语言模型的能力进行RAG评估**</u>。
2. **简短修订说明**：
    - **术语统一情况**：将“IG”统一为“RAG”，“大元模型”统一为“大语言模型”。
    - **主要口误修正点**：删除冗余表述“相对来说说”；修正“孕妇提出个问题”为“用户提出个问题” ；修正“这个上下文信息不能为回答这个问题提供有价值的信息”前面对应描述，使其逻辑一致。 
