# 8-5 实战：用Ragas评估制度问答模块的性能_ev- 笔记

在构建完**RAG系统**之后，我们需要对**RAG系统**进行全面评估，以确定其性能是否符合要求。除了人工评估以外，本节我们使用**RAGAS**这个自动评估框架对**RAG系统**进行评估。

在开始评估之前，我们重新构建之前搭建的**RAG系统**，来构建一个制度问答助手的**RAG pipeline**。同样地，我们引入**embedding模型**，还有两个**大语言模型**，以及**Chromadb**数据库检索的包和方法。接着创建**大语言模型**的实例，以及准备好**prompt模板**。

接下来，我们把向量数据库**Chromadb**中的知识库加载进来。首先建立与**Chromadb向量数据库**的连接，然后设置**embedding模型**，接着通过之前构建好的向量集合“制度DB”，利用**Chromadb**类加载这个向量集合，也就是我们的知识库。在此过程中，需要传入**Chromadb的连接**以及**embedding函数**。

有了知识库以后，我们就可以构建一个问答的**pipeline**。在这个**pipeline**中，给定一个问题，同样会在知识库中检索**K个相关的文档**。然后把这**K个相关的文档**作为上下文信息拼接在一起，与问题一同放入**prompt模板**中，再输入给**大语言模型**来生成答案。由于在**RAGAS评估过程**中需要问题的上下文信息，所以在这个**pipeline**中，我们也将问题相关的上下文信息列表一并返回。至此，我们构建完了一个**RAG的pipeline**。

有了这个**pipeline**以后，我们就可以开始实现**RAGAS的评估**。**RAGAS评估**的第一步是构建评估数据集。我们知道，**RAGAS的评估数据集**包含四个部分：
    - 第一个是问题。
    - 第二个是这个问题的标准答案。
    - 第三个是这个问题检索到的上下文信息。
    - 第四个是**大语言模型**对这个问题生成的答案。

这四个部分中，前两个部分需要我们手动提供，而后两个部分，即上下文信息和生成的答案，由刚才构建的**RAG pipeline**生成。

首先，我们给出问题及其标准答案。例如，我们给出两个问题，以及这两个问题对应的标准答案。接下来，我们需要将问题输入到**RAG pipeline**中，以获取其上下文信息以及生成的答案。我们遍历问题列表中的每个问题，将问题放入上面构建好的**RAG pipeline**中，并设定返回的上下文信息数量为三个。这样，我们就能得到每个问题生成的答案，以及该问题在知识库中检索到的上下文信息列表。我们将这两个信息分别放入“生成的答案”和“上下文列表”中，然后执行操作。如此，我们就完成了生成答案和上下文信息的数据收集。

接下来，我们把这四个部分整合到一个**dataset**中。先将这四个部分放入一个字典里，即问题、问题生成的答案、问题的上下文信息以及问题对应的标准答案（**GT**）。然后通过这些信息构建一个**dataset对象**，这个**dataset对象**就是符合**RAGAS评估要求的数据集**。

有了数据集以后，我们要选择评估指标。这里我们选取四个最常用的**RAGAS评估指标**：
    - 第一个是**忠实性指标**。
    - 第二个是**答案相关度指标**。
    - 剩下两个是评估上下文信息的**上下文召回指标**和**上下文精度指标**。

接下来，我们执行**RAGAS的评估**。首先导入评估方法以及一些配置，因为在**RAGAS评估过程**中会使用到**大语言模型**以及**embedding模型**。同样地，我们先实例化这两种模型，这里的**大语言模型**选用千问2 - 72B模型，**embedding模型**与**RAG pipeline**中的**embedding模型**一样，是**BGE - M3 - 100模型**。由于在**RAG评估过程**中需要多次调用**大语言模型**，耗时可能较长，所以我们通过配置文件适当延长它的超时时间。

有了这些准备后，我们就可以通过**evaluate**函数进行整个**RAGAS的评估**。在**ragas.evaluate**这个函数中，我们需要传入评估数据集、要调用的**大语言模型**、**embedding函数**、要评估的指标以及一些配置信息。最后将结果转换成表格形式输出。现在我们来执行一下。

已经执行完了，我们来看一下结果。从这个结果可以看出，针对这些问题检索出来的上下文信息质量较高。不过，忠实性指标在第二个问题上表现稍差，答案相关性指标在第二个问题上也稍差。

<u>有了这些评估结果以后，我们就可以有针对性地优化**RAG系统**。例如，对于这些指标不太好的部分，我们可以分析具体原因，然后进行有针对性的优化。</u>当然，在本次演示中我们只问了两个问题。在实际项目中，我们需要生成更多问题，以覆盖用户提问的不同场景。这些问题我们也可以借助**大语言模型的能力**自动生成。这样，我们就完成了一次**RAGAS的自动评估**。

2. **简短修订说明**：
    - **术语统一情况**：将“IG”统一修订为“RAG”，“IGAS”统一修订为“RAGAS”，“ebi模型”修订为“embedding模型”，“单元模型”修订为“大语言模型”，“克罗玛”修订为“Chromadb”，“提示时”修订为“prompt”，“ebel ding模型”修订为“embedding模型”。
    - **主要口误修正点**：修正了多处表述不清晰、重复及口语化的问题。例如，“然后或是应该模型”改为“然后设置embedding模型”；“把这两个信息放到生成的答案和上下文列表面去”改为“将这两个信息分别放入‘生成的答案’和‘上下文列表’中”；“ig as”改为“RAGAS”；“ig s evaluate”改为“ragas.evaluate”；“100 0模型”改为“BGE - M3 - 100模型”等，使文稿表述更清晰准确，符合技术文档规范。 